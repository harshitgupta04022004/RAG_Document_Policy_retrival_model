{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3634f1d-de5f-43e3-be01-8cecf460c458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# Data Processing\n",
    "import pypdf\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Embeddings and Vector Store\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# Graph Database\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Language Model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0b59038-4884-4e39-80a4-4815262852b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_TOKEN=\"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\"\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "HF_TOKEN = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68b79cd7-bf76-4ca3-a6ba-193f581dd55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "NEO4J_URI='neo4j+s://fdb1cdfe.databases.neo4j.io'\n",
    "NEO4J_USERNAME='neo4j'\n",
    "NEO4J_PASSWORD='4ygC6vXH3auM-yPJ8XW1oUjHQDSJCL0IXCSAK0xKUF4'\n",
    "NEO4J_DATABASE='neo4j'\n",
    "AURA_INSTANCEID='fdb1cdfe'\n",
    "AURA_INSTANCENAME='Free instance'\n",
    "!export HF_TOKEN=\"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\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class InsuranceRAGSystem:\n",
    "    \"\"\"\n",
    "    A comprehensive RAG (Retrieval-Augmented Generation) system for insurance policy analysis.\n",
    "    Combines vector search (ChromaDB) and graph database (Neo4j) for document retrieval,\n",
    "    with DeepSeek model for intelligent response generation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 neo4j_uri: str,\n",
    "                 neo4j_username: str, \n",
    "                 neo4j_password: str,\n",
    "                 neo4j_database: str = \"neo4j\",\n",
    "                 hf_token: str = \"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\",\n",
    "                 chromadb_path: str = \"doc_db\",\n",
    "                 embedding_model: str = \"Qwen/Qwen3-Embedding-0.6B\",\n",
    "                 llm_model: str = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"):\n",
    "        \"\"\"\n",
    "        Initialize the Insurance RAG System.\n",
    "        \n",
    "        Args:\n",
    "            neo4j_uri: Neo4j database URI\n",
    "            neo4j_username: Neo4j username\n",
    "            neo4j_password: Neo4j password\n",
    "            neo4j_database: Neo4j database name\n",
    "            hf_token: Hugging Face token\n",
    "            chromadb_path: Path for ChromaDB persistence\n",
    "            embedding_model: Name of the embedding model\n",
    "            llm_model: Name of the language model\n",
    "        \"\"\"\n",
    "        # Configuration\n",
    "        self.neo4j_uri = NEO4J_URI\n",
    "        self.neo4j_username = NEO4J_USERNAME\n",
    "        self.neo4j_password = NEO4J_PASSWORD\n",
    "        self.neo4j_database = NEO4J_DATABASE\n",
    "        self.chromadb_path = 'doc_db'\n",
    "        self.embedding_model_name = 'Qwen/Qwen3-Embedding-0.6B'\n",
    "        self.llm_model_name = \"microsoft/Phi-3-mini-128k-instruct\"#\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "        \n",
    "        # Model configurations\n",
    "        self.embedding_dimension = 1024\n",
    "        self.vector_index_name = \"chunkEmbeddings\"\n",
    "        self.collection_name = \"document_chunks_collection\"\n",
    "        \n",
    "        # Initialize components\n",
    "        self.embedding_model = None\n",
    "        self.tokenizer = None\n",
    "        self.llm_model = None\n",
    "        self.chromadb_client = chromadb.PersistentClient(path=\"doc_db\")\n",
    "        self.chromadb_collection = self.chromadb_client.get_or_create_collection(name=\"document_chunks_collection\")\n",
    "        self.neo4j_driver = GraphDatabase.driver(self.neo4j_uri,auth=(self.neo4j_username,  self.neo4j_password))           # e.g., 'neo4j+s://fdb1cdfe.databases.neo4j.io'auth=(username, password)  # Neo4j credentials)\n",
    "        self.chunks = []\n",
    "        self.embeddings = []\n",
    "        \n",
    "        # Set HF token if provided\n",
    "        if hf_token:\n",
    "            os.environ[\"HF_TOKEN\"] = hf_token\n",
    "        \n",
    "        # Initialize device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Initialize all components\n",
    "        self._initialize_models()\n",
    "        self._initialize_databases()\n",
    "    \n",
    "    def _initialize_models(self):\n",
    "        \"\"\"Initialize embedding and language models.\"\"\"\n",
    "        print(\"Initializing models...\")\n",
    "        \n",
    "        # Initialize embedding model\n",
    "        self.embedding_model = SentenceTransformer(self.embedding_model_name, device=\"cpu\")\n",
    "        print(f\"✓ Embedding model loaded: {self.embedding_model_name}\")\n",
    "        \n",
    "        # Initialize tokenizer and LLM\n",
    "        self.tokenizer =  AutoTokenizer.from_pretrained(self.llm_model_name)\n",
    "        self.llm_model = AutoModelForCausalLM.from_pretrained(self.llm_model_name,torch_dtype=torch.float32, device_map=\"auto\")\n",
    "        print(f\"✓ Language model loaded: {self.llm_model_name}\")\n",
    "    \n",
    "    def _initialize_databases(self):\n",
    "        \"\"\"Initialize ChromaDB and Neo4j connections.\"\"\"\n",
    "        print(\"Initializing databases...\")\n",
    "        \n",
    "        # Initialize ChromaDB\n",
    "        self.chromadb_client = chromadb.PersistentClient(path=self.chromadb_path)\n",
    "        self.chromadb_collection = self.chromadb_client.get_or_create_collection(\n",
    "            name=self.collection_name\n",
    "        )\n",
    "        print(f\"✓ ChromaDB initialized: {self.collection_name}\")\n",
    "        \n",
    "        # Initialize Neo4j\n",
    "        try:\n",
    "            self.neo4j_driver = GraphDatabase.driver(\n",
    "                self.neo4j_uri, \n",
    "                auth=(self.neo4j_username, self.neo4j_password)\n",
    "            )\n",
    "            self.neo4j_driver.verify_connectivity()\n",
    "            print(\"✓ Neo4j connection established\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Neo4j connection failed: {e}\")\n",
    "            self.neo4j_driver = None\n",
    "    \n",
    "    def extract_text_from_pdf(self, pdf_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract text from a PDF file.\n",
    "        \n",
    "        Args:\n",
    "            pdf_path: Path to the PDF file\n",
    "            \n",
    "        Returns:\n",
    "            Extracted text as string\n",
    "        \"\"\"\n",
    "        text = \"\"\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                reader = pypdf.PdfReader(file)\n",
    "                for page_num in range(len(reader.pages)):\n",
    "                    page = reader.pages[page_num]\n",
    "                    text += page.extract_text()\n",
    "            print(f\"✓ Extracted text from PDF: {pdf_path}\")\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error extracting PDF: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def split_text_into_chunks(self, text: str, chunk_size: int = 256, chunk_overlap: int = 32) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split text into chunks using RecursiveCharacterTextSplitter.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to split\n",
    "            chunk_size: Size of each chunk\n",
    "            chunk_overlap: Overlap between chunks\n",
    "            \n",
    "        Returns:\n",
    "            List of text chunks\n",
    "        \"\"\"\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            is_separator_regex=False\n",
    "        )\n",
    "        \n",
    "        documents = text_splitter.create_documents([text])\n",
    "        self.chunks = [doc.page_content for doc in documents]\n",
    "        print(f\"✓ Split text into {len(self.chunks)} chunks\")\n",
    "        return self.chunks\n",
    "    \n",
    "    def generate_embeddings(self, chunks: List[str] = None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for text chunks.\n",
    "        \n",
    "        Args:\n",
    "            chunks: List of text chunks (uses self.chunks if None)\n",
    "            \n",
    "        Returns:\n",
    "            NumPy array of embeddings\n",
    "        \"\"\"\n",
    "        if chunks is None:\n",
    "            chunks = self.chunks\n",
    "        \n",
    "        if not chunks:\n",
    "            print(\"✗ No chunks available for embedding generation\")\n",
    "            return np.array([])\n",
    "        \n",
    "        self.embeddings = self.embedding_model.encode(chunks, convert_to_tensor=False)\n",
    "        print(f\"✓ Generated {len(self.embeddings)} embeddings\")\n",
    "        print(f\"✓ Embedding shape: {self.embeddings[0].shape}\")\n",
    "        return self.embeddings\n",
    "    \n",
    "    def store_in_chromadb(self, chunks: List[str] = None, embeddings: np.ndarray = None):\n",
    "        \"\"\"\n",
    "        Store chunks and embeddings in ChromaDB.\n",
    "        \n",
    "        Args:\n",
    "            chunks: List of text chunks\n",
    "            embeddings: Array of embeddings\n",
    "        \"\"\"\n",
    "        if chunks is None:\n",
    "            chunks = self.chunks\n",
    "        if embeddings is None:\n",
    "            embeddings = self.embeddings\n",
    "        \n",
    "        if len(chunks) == 0 or len(embeddings) == 0:\n",
    "            print(\"✗ No data to store in ChromaDB\")\n",
    "            return\n",
    "        \n",
    "        # Generate unique IDs\n",
    "        ids = [f\"chunk_{i}\" for i in range(len(chunks))]\n",
    "        \n",
    "        # Add to collection\n",
    "        self.chromadb_collection.add(\n",
    "            ids=ids,\n",
    "            documents=chunks,\n",
    "            embeddings=embeddings.tolist()\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Stored {self.chromadb_collection.count()} items in ChromaDB\")\n",
    "    \n",
    "    def store_in_neo4j(self, chunks: List[str] = None, embeddings: np.ndarray = None):\n",
    "        \"\"\"\n",
    "        Store chunks and embeddings in Neo4j graph database.\n",
    "        \n",
    "        Args:\n",
    "            chunks: List of text chunks\n",
    "            embeddings: Array of embeddings\n",
    "        \"\"\"\n",
    "        if not self.neo4j_driver:\n",
    "            print(\"✗ Neo4j driver not available\")\n",
    "            return\n",
    "        \n",
    "        if chunks is None:\n",
    "            chunks = self.chunks\n",
    "        if embeddings is None:\n",
    "            embeddings = self.embeddings\n",
    "        \n",
    "        if len(chunks) == 0 or len(embeddings) == 0:\n",
    "            print(\"✗ No data to store in Neo4j\")\n",
    "            return\n",
    "        \n",
    "        with self.neo4j_driver.session(database=self.neo4j_database) as session:\n",
    "            # Create constraint\n",
    "            session.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (c:Chunk) REQUIRE c.id IS UNIQUE\")\n",
    "            \n",
    "            # Create chunk nodes\n",
    "            for i, (text, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "                params = {\n",
    "                    'id': f\"chunk_{i}\",\n",
    "                    'text': text,\n",
    "                    'embedding': embedding.tolist(),\n",
    "                    'chunk_index': i\n",
    "                }\n",
    "                session.run(\"\"\"\n",
    "                    MERGE (c:Chunk {id: $id})\n",
    "                    ON CREATE SET c.text = $text, c.embedding = $embedding, c.chunkIndex = $chunk_index\n",
    "                \"\"\", params)\n",
    "            \n",
    "            # Create relationships between consecutive chunks\n",
    "            for i in range(len(chunks) - 1):\n",
    "                params = {'current_id': f\"chunk_{i}\", 'next_id': f\"chunk_{i+1}\"}\n",
    "                session.run(\"\"\"\n",
    "                    MATCH (current:Chunk {id: $current_id})\n",
    "                    MATCH (next:Chunk {id: $next_id})\n",
    "                    MERGE (current)-[:NEXT]->(next)\n",
    "                \"\"\", params)\n",
    "        \n",
    "        print(f\"✓ Stored {len(chunks)} chunks in Neo4j\")\n",
    "        self._create_neo4j_vector_index()\n",
    "    \n",
    "    def _create_neo4j_vector_index(self):\n",
    "        \"\"\"Create vector index in Neo4j for similarity search.\"\"\"\n",
    "        if not self.neo4j_driver:\n",
    "            return\n",
    "        \n",
    "        with self.neo4j_driver.session(database=self.neo4j_database) as session:\n",
    "            # Check if index exists\n",
    "            result = session.run(\n",
    "                \"SHOW INDEXES YIELD name WHERE name = $index_name\", \n",
    "                index_name=self.vector_index_name\n",
    "            )\n",
    "            \n",
    "            if result.single():\n",
    "                print(f\"✓ Vector index '{self.vector_index_name}' already exists\")\n",
    "                return\n",
    "            \n",
    "            # Create vector index\n",
    "            query = f\"\"\"\n",
    "            CREATE VECTOR INDEX `{self.vector_index_name}` IF NOT EXISTS\n",
    "            FOR (c:Chunk) ON (c.embedding)\n",
    "            OPTIONS {{ indexConfig: {{\n",
    "                `vector.dimensions`: {self.embedding_dimension},\n",
    "                `vector.similarity_function`: 'cosine'\n",
    "            }}\n",
    "            }}\n",
    "            \"\"\"\n",
    "            \n",
    "            try:\n",
    "                session.run(query)\n",
    "                print(f\"✓ Created vector index '{self.vector_index_name}'\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error creating vector index: {e}\")\n",
    "    \n",
    "    def retrieve_from_chromadb(self, query_text: str, k: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve top-k similar chunks from ChromaDB.\n",
    "        \n",
    "        Args:\n",
    "            query_text: Query text\n",
    "            k: Number of results to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            List of retrieved chunks with metadata\n",
    "        \"\"\"\n",
    "        query_embedding = self.embedding_model.encode([query_text], convert_to_tensor=True)\n",
    "        \n",
    "        results = self.chromadb_collection.query(\n",
    "            query_embeddings=query_embedding.tolist(),\n",
    "            n_results=k\n",
    "        )\n",
    "        \n",
    "        retrieved_chunks = []\n",
    "        for i, chunk in enumerate(results['documents'][0]):\n",
    "            retrieved_chunks.append({\n",
    "                \"clause_id\": f\"C{i+1}\",\n",
    "                \"text\": chunk.strip(),\n",
    "                \"source\": \"chromadb\"\n",
    "            })\n",
    "        \n",
    "        print(f\"✓ Retrieved {len(retrieved_chunks)} chunks from ChromaDB\")\n",
    "        return retrieved_chunks\n",
    "    \n",
    "    def retrieve_from_neo4j(self, query_text: str, k: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve top-k similar chunks from Neo4j using vector similarity.\n",
    "        \n",
    "        Args:\n",
    "            query_text: Query text\n",
    "            k: Number of results to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            List of retrieved chunks with similarity scores\n",
    "        \"\"\"\n",
    "        if not self.neo4j_driver:\n",
    "            print(\"✗ Neo4j driver not available\")\n",
    "            return []\n",
    "        \n",
    "        query_embedding = self.embedding_model.encode(query_text).tolist()\n",
    "        \n",
    "        cypher_query = f\"\"\"\n",
    "        CALL db.index.vector.queryNodes('{self.vector_index_name}', $k, $embedding)\n",
    "        YIELD node, score\n",
    "        RETURN node.text AS text, score\n",
    "        \"\"\"\n",
    "        \n",
    "        with self.neo4j_driver.session(database=self.neo4j_database) as session:\n",
    "            try:\n",
    "                result = session.run(cypher_query, k=k, embedding=query_embedding)\n",
    "                chunks = []\n",
    "                for i, record in enumerate(result):\n",
    "                    chunks.append({\n",
    "                        \"clause_id\": f\"C{i+1}\",\n",
    "                        \"text\": record[\"text\"].strip(),\n",
    "                        \"score\": record[\"score\"],\n",
    "                        \"source\": \"neo4j\"\n",
    "                    })\n",
    "                \n",
    "                print(f\"✓ Retrieved {len(chunks)} chunks from Neo4j\")\n",
    "                return chunks\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error querying Neo4j: {e}\")\n",
    "                return []\n",
    "    \n",
    "    def hybrid_retrieve(self, query_text: str, k: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve chunks using both ChromaDB and Neo4j, then merge results.\n",
    "        \n",
    "        Args:\n",
    "            query_text: Query text\n",
    "            k: Number of results to retrieve from each source\n",
    "            \n",
    "        Returns:\n",
    "            Merged list of retrieved chunks\n",
    "        \"\"\"\n",
    "        chromadb_results = self.retrieve_from_chromadb(query_text, k)\n",
    "        neo4j_results = self.retrieve_from_neo4j(query_text, k)\n",
    "        \n",
    "        # Simple merge (could be enhanced with deduplication)\n",
    "        all_results = chromadb_results + neo4j_results\n",
    "        \n",
    "        # Re-index clause IDs\n",
    "        for i, result in enumerate(all_results):\n",
    "            result[\"clause_id\"] = f\"C{i+1}\"\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def generate_response(self, query_text: str, retrieved_chunks: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"\n",
    "        Generate response using the DeepSeek model with retrieved context.\n",
    "        \n",
    "        Args:\n",
    "            query_text: User query\n",
    "            retrieved_chunks: Retrieved relevant chunks\n",
    "            \n",
    "        Returns:\n",
    "            Generated response as JSON string\n",
    "        \"\"\"\n",
    "        retrieved_clauses_json_str = json.dumps(retrieved_chunks, indent=2)\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are an expert insurance-policy decision assistant with deep domain knowledge in maternity and routine-care covers. You process natural-language queries by strictly analyzing **only** the user's input and the provided policy clauses—no invented rules.\n",
    "\n",
    "Your task:\n",
    "1. Parse the user's query and extract structured information.\n",
    "2. Evaluate eligibility **using only** the retrieved clauses—do not assume or invent any additional rules.\n",
    "3. Return a final decision and justification by referencing the exact clause IDs.\n",
    "\n",
    "——\n",
    "\n",
    "USER QUERY:\n",
    "\"{query_text}\"\n",
    "\n",
    "——\n",
    "\n",
    "RETRIEVED CLAUSES (from internal document search):\n",
    "{retrieved_clauses_json_str}\n",
    "\n",
    "——\n",
    "\n",
    "Step 1: Extract these fields from the query:\n",
    "- age (number)\n",
    "- gender\n",
    "- procedure\n",
    "- city\n",
    "- policy_duration_months\n",
    "- selected_cover_option (i, ii, or iii)\n",
    "\n",
    "Step 2: Using **only** the clauses above:\n",
    "- Determine if the requested procedure/cover is eligible under the selected option.\n",
    "- Identify any exclusions.\n",
    "- Compute payout amount (if applicable).\n",
    "\n",
    "Step 3: Output exactly and only this JSON (no extra text), then immediately stop:\n",
    "{{\n",
    "  \"decision\": \"<approved|rejected>\",\n",
    "  \"amount\": <number|null>,\n",
    "  \"justification\": [\n",
    "    {{\n",
    "      \"clause_id\": \"C1\",\n",
    "      \"reason\": \"…\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "<<END_OF_JSON>>\n",
    "\"\"\"\n",
    "        \n",
    "        # Generate response\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        outputs = self.llm_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "            do_sample=False,\n",
    "            num_beams=3,\n",
    "        )\n",
    "        \n",
    "        raw_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract JSON response\n",
    "        try:\n",
    "            json_response = raw_response.split(\"<<END_OF_JSON>>\")[0].split(prompt)[1].strip()\n",
    "            return json_response\n",
    "        except:\n",
    "            return raw_response\n",
    "    \n",
    "    def process_documents(self, pdf_paths: List[str], chunk_size: int = 256, chunk_overlap: int = 32):\n",
    "        \"\"\"\n",
    "        Complete pipeline to process PDF documents and store in both databases.\n",
    "        \n",
    "        Args:\n",
    "            pdf_paths: List of PDF file paths\n",
    "            chunk_size: Size of text chunks\n",
    "            chunk_overlap: Overlap between chunks\n",
    "        \"\"\"\n",
    "        print(\"Starting document processing pipeline...\")\n",
    "        \n",
    "        # Extract text from all PDFs\n",
    "        all_text = \"\"\n",
    "        for pdf_path in pdf_paths:\n",
    "            text = self.extract_text_from_pdf(pdf_path)\n",
    "            all_text += text + \"\\n\\n\"\n",
    "        \n",
    "        if not all_text.strip():\n",
    "            print(\"✗ No text extracted from PDFs\")\n",
    "            return\n",
    "        \n",
    "        # Split into chunks\n",
    "        chunks = self.split_text_into_chunks(all_text, chunk_size, chunk_overlap)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = self.generate_embeddings(chunks)\n",
    "        \n",
    "        # Store in both databases\n",
    "        self.store_in_chromadb(chunks, embeddings)\n",
    "        self.store_in_neo4j(chunks, embeddings)\n",
    "        \n",
    "        print(\"✓ Document processing pipeline completed\")\n",
    "    \n",
    "    def query(self, query_text: str, retrieval_method: str = \"hybrid\", k: int = 3) -> str:\n",
    "        \"\"\"\n",
    "        End-to-end query processing: retrieve relevant chunks and generate response.\n",
    "        \n",
    "        Args:\n",
    "            query_text: User query\n",
    "            retrieval_method: \"chromadb\", \"neo4j\", or \"hybrid\"\n",
    "            k: Number of chunks to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Generated response\n",
    "        \"\"\"\n",
    "        print(f\"Processing query: '{query_text}'\")\n",
    "        \n",
    "        # Retrieve relevant chunks\n",
    "        if retrieval_method == \"chromadb\":\n",
    "            retrieved_chunks = self.retrieve_from_chromadb(query_text, k)\n",
    "        elif retrieval_method == \"neo4j\":\n",
    "            retrieved_chunks = self.retrieve_from_neo4j(query_text, k)\n",
    "        elif retrieval_method == \"hybrid\":\n",
    "            retrieved_chunks = self.hybrid_retrieve(query_text, k//2 + 1)\n",
    "        else:\n",
    "            raise ValueError(\"retrieval_method must be 'chromadb', 'neo4j', or 'hybrid'\")\n",
    "        \n",
    "        # Generate response\n",
    "        response = self.generate_response(query_text, retrieved_chunks)\n",
    "        \n",
    "        print(\"✓ Query processing completed\")\n",
    "        return response\n",
    "    \n",
    "    def close_connections(self):\n",
    "        \"\"\"Close database connections.\"\"\"\n",
    "        if self.neo4j_driver:\n",
    "            self.neo4j_driver.close()\n",
    "            print(\"✓ Neo4j connection closed\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Configuration\n",
    "#     NEO4J_URI = 'neo4j+s://fdb1cdfe.databases.neo4j.io'\n",
    "#     NEO4J_USERNAME = 'neo4j'\n",
    "#     NEO4J_PASSWORD = '4ygC6vXH3auM-yPJ8XW1oUjHQDSJCL0IXCSAK0xKUF4'\n",
    "#     HF_TOKEN = \"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\"\n",
    "    \n",
    "# # Initialize the RAG system\n",
    "# rag_system = InsuranceRAGSystem(\n",
    "#     neo4j_uri=NEO4J_URI,\n",
    "#     neo4j_username=NEO4J_USERNAME,\n",
    "#     neo4j_password=NEO4J_PASSWORD,\n",
    "#     hf_token=HF_TOKEN\n",
    "# )\n",
    "\n",
    "# # Process documents\n",
    "# pdf_files = ['./CHOTGDP23004V012223.pdf']  # Add your PDF paths here\n",
    "# rag_system.process_documents(pdf_files)\n",
    "\n",
    "# # Query examples\n",
    "# queries = [\n",
    "#     \"What is the definition of Burglary in the insurance policy?\",\n",
    "#     \"What is covered under accidental death in this policy?\",\n",
    "#     \"Who qualifies as a family member under the family travel policy?\"\n",
    "# ]\n",
    "\n",
    "# for query in queries:\n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(f\"Query: {query}\")\n",
    "#     print(f\"{'='*60}\")\n",
    "#     response = rag_system.query(query)\n",
    "#     print(\"Response:\", response)\n",
    "\n",
    "# # Clean up\n",
    "# rag_system.close_connections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd60a487-2702-40f0-9a19-3f03f092dba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Initializing models...\n",
      "✓ Embedding model loaded: Qwen/Qwen3-Embedding-0.6B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af66dfc808f14b479e2c98054ef38028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a5e2aaa87a4d79a4f821f33ecd9d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f16fbd7a61d4ca698b80a85faa3e2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb8dc7cefb9477ebde2db52dedb406e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab65f493bc34a87b6190384eb0fc60e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bede3cb82e194cafa429c28a7102af48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ae315d4bb049638feabe4edfb66b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e830613dc1f44cb58ba92966003e1978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c04384d6986d431499983f1544818ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c05bfdb3d64c45049a86dca8df57450e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588daab525714e04b545dea32aa73c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23f3efe4fff4e6e8c88b5074830f97d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Language model loaded: microsoft/Phi-3-mini-128k-instruct\n",
      "Initializing databases...\n",
      "✓ ChromaDB initialized: document_chunks_collection\n",
      "✓ Neo4j connection established\n"
     ]
    }
   ],
   "source": [
    "rag_system = InsuranceRAGSystem(\n",
    "    neo4j_uri=NEO4J_URI,\n",
    "    neo4j_username=NEO4J_USERNAME,\n",
    "    neo4j_password=NEO4J_PASSWORD,\n",
    "    hf_token=HF_TOKEN\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8786854-5596-4d2f-8f64-ee0b7c87f369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting document processing pipeline...\n",
      "✓ Extracted text from PDF: ./CHOTGDP23004V012223.pdf\n",
      "✓ Split text into 1699 chunks\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Process documents\u001b[39;00m\n\u001b[1;32m      2\u001b[0m pdf_files \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./CHOTGDP23004V012223.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Add your PDF paths here\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mrag_system\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_files\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 495\u001b[0m, in \u001b[0;36mInsuranceRAGSystem.process_documents\u001b[0;34m(self, pdf_paths, chunk_size, chunk_overlap)\u001b[0m\n\u001b[1;32m    492\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_text_into_chunks(all_text, chunk_size, chunk_overlap)\n\u001b[1;32m    494\u001b[0m \u001b[38;5;66;03m# Generate embeddings\u001b[39;00m\n\u001b[0;32m--> 495\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;66;03m# Store in both databases\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore_in_chromadb(chunks, embeddings)\n",
      "Cell \u001b[0;32mIn[3], line 179\u001b[0m, in \u001b[0;36mInsuranceRAGSystem.generate_embeddings\u001b[0;34m(self, chunks)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✗ No chunks available for embedding generation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ Generated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ Embedding shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen_env/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen_env/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:1052\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m-> 1052\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1054\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen_env/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:1133\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m             module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[1;32m   1128\u001b[0m         module_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1129\u001b[0m             key: value\n\u001b[1;32m   1130\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1131\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mforward_kwargs)\n\u001b[1;32m   1132\u001b[0m         }\n\u001b[0;32m-> 1133\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen_env/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:437\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[1;32m    431\u001b[0m trans_features \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    432\u001b[0m     key: value\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    435\u001b[0m }\n\u001b[0;32m--> 437\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m token_embeddings \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    439\u001b[0m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token_embeddings\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen_env/lib/python3.10/site-packages/transformers/utils/generic.py:1083\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m                 module\u001b[38;5;241m.\u001b[39mforward \u001b[38;5;241m=\u001b[39m make_capture_wrapper(module, original_forward, key, specs\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   1081\u001b[0m                 monkey_patched_layers\u001b[38;5;241m.\u001b[39mappend((module, original_forward))\n\u001b[0;32m-> 1083\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;66;03m# Restore original forward methods\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module, original_forward \u001b[38;5;129;01min\u001b[39;00m monkey_patched_layers:\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen_env/lib/python3.10/site-packages/transformers/models/qwen3/modeling_qwen3.py:405\u001b[0m, in \u001b[0;36mQwen3Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(hidden_states, position_ids)\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers]:\n\u001b[0;32m--> 405\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(hidden_states)\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[1;32m    418\u001b[0m     last_hidden_state\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    419\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    420\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen_env/lib/python3.10/site-packages/transformers/modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen_env/lib/python3.10/site-packages/transformers/models/qwen3/modeling_qwen3.py:257\u001b[0m, in \u001b[0;36mQwen3DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m hidden_states, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen_env/lib/python3.10/site-packages/transformers/models/qwen3/modeling_qwen3.py:227\u001b[0m, in \u001b[0;36mQwen3Attention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m attn_output, attn_weights \u001b[38;5;241m=\u001b[39m attention_interface(\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    216\u001b[0m     query_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    224\u001b[0m )\n\u001b[1;32m    226\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m--> 227\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mo_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, attn_weights\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen_env/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Process documents\n",
    "pdf_files = ['./CHOTGDP23004V012223.pdf']  # Add your PDF paths here\n",
    "rag_system.process_documents(pdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e25ef92-f43c-46c2-9af5-4e18f9402c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Query examples\n",
    "queries = [\n",
    "    \"What is the definition of Burglary in the insurance policy?\",\n",
    "    \"What is covered under accidental death in this policy?\",\n",
    "    \"Who qualifies as a family member under the family travel policy?\",\n",
    "    \"What are the exclusions under trip cancellation benefits?\",\n",
    "    \"Does the policy cover hospitalization due to COVID-19?\",\n",
    "    \"What is meant by deductible in this policy?\",\n",
    "    \"Explain the conditions under which repatriation of remains is covered.\",\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    response = rag_system.query(query)\n",
    "    print(\"Response:\", response)\n",
    "\n",
    "# Clean up\n",
    "rag_system.close_connections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81d5b07-7228-4524-8a61-c68b15a9d645",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
