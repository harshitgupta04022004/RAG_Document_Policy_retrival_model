{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7cb8441-f4f3-4e3d-91c3-78e2031668d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_URI='neo4j+s://fdb1cdfe.databases.neo4j.io'\n",
    "NEO4J_USERNAME='neo4j'\n",
    "NEO4J_PASSWORD='4ygC6vXH3auM-yPJ8XW1oUjHQDSJCL0IXCSAK0xKUF4'\n",
    "NEO4J_DATABASE='neo4j'\n",
    "AURA_INSTANCEID='fdb1cdfe'\n",
    "AURA_INSTANCENAME='Free instance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e44f1ea9-ed0d-414c-a37f-1c6f41e47d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-02 19:49:16 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
    "import torch\n",
    "import pypdf\n",
    "from Pleias_Rag.RagSystem import RagSystem\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = \"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\" \n",
    "# !export HF_TOKEN=\"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38b12745-e9aa-42af-897d-76c71339a509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PyPDF2\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = pypdf.PdfReader(file)\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b96e99e4-2f46-4931-ad99-c38a2f472514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_1_text = extract_text_from_pdf('./BAJHLIP23020V012223.pdf')\n",
    "pdf_2_text = extract_text_from_pdf('./EDLHLGA23009V012223.pdf')\n",
    "# pdf_3_text = extract_text_from_pdf('./ICIHLIP22012V012223.pdf')\n",
    "# pdf_4_text = extract_text_from_pdf('./CHOTGDP23004V012223.pdf')\n",
    "# pdf_5_text = extract_text_from_pdf('./HDFHLIP23024V072223.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac06172d-0e42-4e47-ae7c-51ac8a3d781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = \"\\n\\n\" + pdf_2_text # + \"\\n\\n\" + pdf_2_text + \"\\n\\n\" + pdf_3_text + \"\\n\\n\" + pdf_4_text + \"\\n\\n\" + pdf_5_text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83b163df-b07c-44a9-af6a-d32fe54b07b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size = 300,\n",
    "#     chunk_overlap = 32,\n",
    "#     is_separator_regex=False\n",
    "# )\n",
    "# Doc = text_splitter.create_documents([documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27bebde8-a1c0-4da5-8b6f-446ea5f89b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6ea87d8-d5b2-42b1-89db-227d9897219d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from Pleias_Rag.RagSystem import RagSystem\n",
    "# emb_model = \"Qwen/Qwen3-Embedding-0.6B\" # 'Qwen/Qwen3-Embedding-0.6B'\n",
    "\n",
    "# # Initialize the RAG system with an optional model path\n",
    "# rag_system = RagSystem(\n",
    "#     search_type=\"vector\",\n",
    "#     db_path=\"data/rag_system_db\",\n",
    "#     embeddings_model=emb_model,\n",
    "#     chunk_size=128,\n",
    "#     # model_path= \"PleIAs/Pleias-RAG-350M\"   # \"PleIAs/Pleias-RAG-1B\"# \"meta-llama/Llama-2-7b-chat-hf\"  # Optional - can also load model later\n",
    "# )\n",
    "\n",
    "\n",
    "# # # You may need to trust remote code for this model\n",
    "# # rag_system.load_model(\n",
    "# #     model_path=\"microsoft/phi-2\",\n",
    "# #     trust_remote_code=True\n",
    "# # )\n",
    "\n",
    "# # model_path=\"microsoft/Phi-3-mini-4k-instruct\"  # <-- The only change needed\n",
    "\n",
    "\n",
    "\n",
    "# # Add documents to the system\n",
    "# rag_system.add_and_chunk_documents([\n",
    "#    documents\n",
    "# ])\n",
    "\n",
    "# # Load the generative model with specific generation parameters to prevent babbling\n",
    "# print(\"Loading generative model with constrained parameters...\")\n",
    "# rag_system.load_model(\n",
    "#     model_path=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "#     max_new_tokens=256,  # <-- Set a strict limit on output length\n",
    "#     temperature=0.1,    # <-- Reduce creativity and repetition\n",
    "#     do_sample=False     # <-- Use greedy decoding\n",
    "# )\n",
    "# print(\"Model loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc766a29-6982-400e-8015-d5ffb0604a6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 42.69 MiB is free. Including non-PyTorch memory, this process has 6.72 GiB memory in use. Of the allocated memory 6.53 GiB is allocated by PyTorch, and 9.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m\n\u001b[1;32m      8\u001b[0m emb_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen3-Embedding-0.6B\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# The embedding model for vector search\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# --- Correcting the 'documents' variable ---\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# The add_and_chunk_documents method expects a list of Document objects.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# We will create a list of Document objects from your PDF text.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Initialize the RAG system correctly\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m rag_system \u001b[38;5;241m=\u001b[39m \u001b[43mRagSystem\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43msearch_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvector\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdb_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/rag_system_db\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memb_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Add documents to the system\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdding and chunking documents...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/hackethon/Rag_Document_policy_model/Pleias-Rag/src/Pleias_Rag/RagSystem.py:59\u001b[0m, in \u001b[0;36mRagSystem.__init__\u001b[0;34m(self, search_type, db_path, embeddings_model, chunk_size, model_path, max_tokens, temperature, top_p, repitition_penalty, trust_remote_code)\u001b[0m\n\u001b[1;32m     56\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(db_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Initialize the RAG database\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb \u001b[38;5;241m=\u001b[39m \u001b[43mRagDatabase\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43msearch_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msearch_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_max_segment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdb_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdb_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings_model\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Generator attributes\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/hackethon/Rag_Document_policy_model/Pleias-Rag/src/Pleias_Rag/RagDatabase.py:61\u001b[0m, in \u001b[0;36mRagDatabase.__init__\u001b[0;34m(self, search_type, default_max_segment, db_path, embeddings_model)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Initialize embeddings model if using vector search\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m search_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvector\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings_model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings_model\u001b[38;5;241m.\u001b[39mget_sentence_embedding_dimension()\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen_env/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:367\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_hpu_graph_enabled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_prompt_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_prompt_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompts:\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1343\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1341\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen_env/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen_env/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 903 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen_env/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen_env/lib/python3.10/site-packages/torch/nn/modules/module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 930\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1329\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1324\u001b[0m             device,\n\u001b[1;32m   1325\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1326\u001b[0m             non_blocking,\n\u001b[1;32m   1327\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1328\u001b[0m         )\n\u001b[0;32m-> 1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 42.69 MiB is free. Including non-PyTorch memory, this process has 6.72 GiB memory in use. Of the allocated memory 6.53 GiB is allocated by PyTorch, and 9.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from Pleias_Rag.RagSystem import RagSystem\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Set your Hugging Face token as an environment variable\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "emb_model = \"Qwen/Qwen3-Embedding-0.6B\" # The embedding model for vector search\n",
    "\n",
    "# --- Correcting the 'documents' variable ---\n",
    "# The add_and_chunk_documents method expects a list of Document objects.\n",
    "# We will create a list of Document objects from your PDF text.\n",
    "# The 'pdf_2_text' variable would be defined in a previous cell,\n",
    "# assuming you extracted it from the PDF using a function like extract_text_from_pdf().\n",
    "# For this example, we'll use a placeholder string.\n",
    "\n",
    "# Initialize the RAG system correctly\n",
    "rag_system = RagSystem(\n",
    "    search_type=\"vector\",\n",
    "    db_path=\"data/rag_system_db\",\n",
    "    embeddings_model=emb_model,\n",
    "    chunk_size=256,\n",
    ")\n",
    "\n",
    "# Add documents to the system\n",
    "print(\"Adding and chunking documents...\")\n",
    "rag_system.add_and_chunk_documents(documents)\n",
    "print(\"Documents added.\")\n",
    "\n",
    "\n",
    "# Load the generative model with specific generation parameters\n",
    "print(\"\\nLoading generative model with constrained parameters...\")\n",
    "# The Pleias-Rag library automatically handles the model download\n",
    "rag_system.load_model(\n",
    "    model_path=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "\n",
    ")\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe5b017-9c5f-4d8a-b796-2df2824dc5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End-to-end RAG query\n",
    "print(\"\\nPerforming RAG query...\")\n",
    "query = \"What is included in routine medical care?\"\n",
    "result = rag_system.query(query)\n",
    "\n",
    "# Access and print the final results\n",
    "print(f\"\\nQuery: {result['query']}\")\n",
    "print(f\"Response: {result['response']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9030b84e-c52e-45ce-908f-462ecc28cab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- 2. Separate Retrieval and Generation ---\n",
    "# query = \"Routine Medical Care would include expenses recommended by a doctor and incurred on –Pharmacy, Diagnostics, Doctor Consultations and Therapy\"\n",
    "# k_responses = 2 # The number of top documents you want to retrieve\n",
    "\n",
    "# # Perform only the retrieval step to get the top 'k' documents\n",
    "# # The .vector_search() method is what you need.\n",
    "# retrieved_results = rag_system.vector_search(query, limit=k_responses)\n",
    "\n",
    "# # Access the results\n",
    "# print(f\"Query: {query}\")\n",
    "# print(f\"Retrieved {k_responses} documents:\")\n",
    "# for i, doc in enumerate(retrieved_results['documents'][0]):\n",
    "#     print(f\"\\nDocument {i+1}:\")\n",
    "#     print(f\"  Content: {doc}\")\n",
    "#     # You can also access the metadata if you stored it.\n",
    "#     # print(f\"  Metadata: {retrieved_results['metadatas'][0][i]}\")\n",
    "\n",
    "# # --- 3. Optional: Use the retrieved results to generate an answer ---\n",
    "# # If you want to continue the RAG process, you would pass these retrieved documents\n",
    "# # to a generation function.\n",
    "# # This part is just for demonstration and is not part of your original query.\n",
    "# # Here we would load the generative model and format the prompt manually.\n",
    "\n",
    "# # print(\"\\n--- Generating a final answer from retrieved documents ---\")\n",
    "# # # Load your generative model\n",
    "# # rag_system.load_model(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "# # # Format the prompt for the generative model\n",
    "# # sources_text = \"\\n\".join([f\"Source {i+1}: {d}\" for i, d in enumerate(retrieved_results['documents'][0])])\n",
    "# # full_prompt = f\"Using the following context, answer the question.\\n\\nContext:\\n{sources_text}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "\n",
    "# # # Generate the final answer\n",
    "# # final_response = rag_system._generate(full_prompt)\n",
    "# # print(f\"Final RAG Answer: {final_response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f174066a-97f7-4f7e-a3a4-cf1e613ad9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from Pleias_Rag.RagSystem import RagSystem\n",
    "from langchain.schema import Document\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# --- 1. Set up Authentication and Load All Models ---\n",
    "# It's best to set this as an environment variable in your terminal.\n",
    "os.environ[\"HF_TOKEN\"] = \"hf_YOUR_TOKEN_HERE\" \n",
    "\n",
    "try:\n",
    "    # Embedding model for vector search\n",
    "    emb_model = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "    \n",
    "    # Pleias model for initial grounded response generation\n",
    "    pleias_rag_model_path = \"PleIAs/Pleias-RAG-350M\"\n",
    "    \n",
    "    # A second, general-purpose LLM for final output formatting\n",
    "    # This model will take the Pleias output and refine it.\n",
    "    formatter_llm_model_path = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "    \n",
    "    # Load the second tokenizer and model for the final step\n",
    "    formatter_tokenizer = AutoTokenizer.from_pretrained(formatter_llm_model_path, token=os.environ[\"HF_TOKEN\"])\n",
    "    formatter_model = AutoModelForCausalLM.from_pretrained(formatter_llm_model_path, token=os.environ[\"HF_TOKEN\"])\n",
    "    \n",
    "    print(\"All models loaded successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading models: {e}\")\n",
    "    print(\"Please ensure your token and permissions are correct.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Initialize and Load the Pleias RAG System ---\n",
    "rag_system = RagSystem(\n",
    "    search_type=\"vector\",\n",
    "    db_path=\"data/rag_system_db\",\n",
    "    embeddings_model=emb_model,\n",
    "    chunk_size=128,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Adding and chunking documents...\")\n",
    "rag_system.add_and_chunk_documents(documents)\n",
    "print(\"Documents added.\")\n",
    "\n",
    "# Load the Pleias generative model\n",
    "print(\"\\nLoading generative model: PleIAs/Pleias-RAG-350M\")\n",
    "rag_system.load_model(\n",
    "    model_path=pleias_rag_model_path,\n",
    "    max_new_tokens=128,\n",
    "    temperature=0.1,\n",
    "    do_sample=False\n",
    ")\n",
    "print(\"Pleias-RAG-350M model loaded.\")\n",
    "\n",
    "\n",
    "# --- 3. First Stage: Perform RAG Query with Pleias ---\n",
    "print(\"\\n--- Stage 1: Performing RAG query with Pleias-RAG-350M ---\")\n",
    "query = \"What is included in routine medical care?\"\n",
    "result_from_pleias = rag_system.query(query)\n",
    "pleias_response = result_from_pleias['response']\n",
    "\n",
    "print(f\"Response from Pleias-RAG-350M:\\n{pleias_response}\")\n",
    "\n",
    "\n",
    "# --- 4. Second Stage: Rephrase and Format with a Second LLM ---\n",
    "print(\"\\n--- Stage 2: Rephrasing response with TinyLlama ---\")\n",
    "\n",
    "# a) Create a new prompt for the second LLM\n",
    "formatting_prompt = f\"\"\"\n",
    "Rephrase the following text as a brief, friendly, and easy-to-read bulleted list.\n",
    "Focus only on the key points.\n",
    "\n",
    "Text to rephrase:\n",
    "{pleias_response}\n",
    "\n",
    "Bulleted list:\n",
    "\"\"\"\n",
    "\n",
    "# b) Generate the final, formatted output\n",
    "inputs = formatter_tokenizer(formatting_prompt, return_tensors=\"pt\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "formatter_model.to(device)\n",
    "inputs.to(device)\n",
    "\n",
    "outputs = formatter_model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,  # A smaller limit for a concise bulleted list\n",
    "    temperature=0.1,\n",
    "    do_sample=True,\n",
    "    pad_token_id=formatter_tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "final_formatted_response = formatter_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# c) Clean up the output to remove the prompt\n",
    "final_formatted_response = final_formatted_response.split(\"Bulleted list:\")[1].strip()\n",
    "\n",
    "print(f\"\\nFinal Formatted Output:\\n{final_formatted_response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
